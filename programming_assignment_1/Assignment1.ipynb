{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Face-Classification Three-Ways\n",
    "\n",
    "In this assignment you will implement a face classifier and train it using three different approaches:\n",
    "* Logistic Regression using Stochastic Gradient Descent\n",
    "* Logistic Regression using Newton's Method\n",
    "* Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First import the required packages and do some setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from assignment1 import sigmoid, cost_function, gradient_function, logistic_SGD, logistic_Newton, gda, predict_function\n",
    "\n",
    "# Set default parameters for plots\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the data set. It consists of 19'832 grayscale images of size 24 x 24. Each image has a corresponding label which we set to 0 for non-face and 1 for face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = loadmat('faces.mat')\n",
    "labels = np.squeeze(data['Labels'])\n",
    "labels[labels == -1] = 0    # Want labels in {0, 1}\n",
    "data = data['Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the dataset into two subsets: One for training and one for testing. This approach is called cross-validation and is standard practice in Machine Learning.\n",
    "The classifier will be learnt only on the data in the training set! The test set then gives you an estimate of how well the classifier will perform on new unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3)\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the images are stored as vectors now. Let's visualize some examples to check that the data is fine. We of course have to reshape the images first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "samples_per_class = 10\n",
    "classes = [0, 1]\n",
    "train_imgs = np.reshape(X_train, [-1, 24, 24], order='F')\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.equal(y_train, cls))\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = y * samples_per_class + i + 1\n",
    "        plt.subplot(len(classes), samples_per_class, plt_idx)\n",
    "        plt.imshow(train_imgs[idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the intercept term by concatenating a vector of ones to the train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept to X and normalize to range [0, 1]\n",
    "X_train = np.concatenate((np.ones((num_train, 1)), X_train/255.), axis=1)\n",
    "X_test = np.concatenate((np.ones((num_test, 1)), X_test/255.), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sigmoid [5 Points]\n",
    "\n",
    "**TODO**: Implement the sigmoid function in ***assignment1/sigmoid.py*** according to the specifications. \n",
    "\n",
    "**NOTE**: The function should work with inputs of arbitrary shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your sigmoid\n",
    "z_test = np.arange(-5, 5, 0.01)\n",
    "g_test = sigmoid(z_test)\n",
    "plt.plot(z_test, g_test)\n",
    "plt.title('Sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Log-Likelihood [10 Points]\n",
    "\n",
    "**TODO**: Implement the log-likelihood for Logistic Regression in ***assignment1/cost_function.py*** according to specs.\n",
    "\n",
    "What value of the cost do you expect with a parameter vector *theta* of all zeros? Check your implementation for this!\n",
    "\n",
    "**Hint**: No for-loops are required! Use np.sum and np.dot instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your cost-function\n",
    "theta_0 = np.zeros(X_train.shape[1])\n",
    "l_0 = cost_function(theta_0, X_train, y_train)\n",
    "print('Log-likelihood with initial theta: ', l_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The Gradient [10 Points]\n",
    "\n",
    "**TODO**: Implement the gradient of the log-likelihood for Logistic Regression in ***assignment1/gradient_function.py*** according to specs. \n",
    "\n",
    "**NOTE**: Your implementation should work with a single example x (i.e., a vector) or multiple examples X (i.e., a matrix).\n",
    "\n",
    "Consider again what value you would expect with *theta* equal to zero and test your implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "x_test = np.ones([2, 10])\n",
    "theta_0 = np.zeros(10)\n",
    "grad_0 = gradient_function(theta_0, x_test, 1.0)\n",
    "print(grad_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exersice 4: The Learning Algorithmms\n",
    "\n",
    "This is the main part of the assignment. Correctness of the implementation is required to get the points (work on speed later). \n",
    "\n",
    "## a) Logistic Regression with SGD [15 Points]\n",
    "\n",
    "**TODO**: Implement the function in ***assignment1/logistic_SGD.py*** according to specs.\n",
    "\n",
    "## b) Logistic Regression with Newton's Method [15 Points]\n",
    "\n",
    "**TODO**: Implement the fuction in ***assignment1/logistic_Newton.py*** according to specs.\n",
    "\n",
    "## c) Gaussian Discriminant Analysis [15 Points]\n",
    "\n",
    "**TODO**: Implement the function in ***assignment1/gda.py*** according to specs.\n",
    "\n",
    "\n",
    "\n",
    "***Hint***: No additional for-loops are required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'sgd'\n",
    "\n",
    "# We'll meausure the execution time\n",
    "start = time.time()\n",
    "\n",
    "if method == 'sgd':\n",
    "    theta, losses = logistic_SGD(X_train, y_train)\n",
    "elif method == 'newton':\n",
    "    theta, losses = logistic_Newton(X_train, y_train)\n",
    "elif method == 'gda':\n",
    "    theta, losses = gda(X_train, y_train)\n",
    "else:\n",
    "    raise ValueError('Method not recognised!')\n",
    "\n",
    "exec_time = time.time()-start\n",
    "print('Total execution time: {}s'.format(exec_time))\n",
    "\n",
    "if losses:\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Log-Likelihood')\n",
    "    plt.show()\n",
    "\n",
    "# We can have a look at what theta has learned to recognise as \"face\"\n",
    "plt.imshow(np.reshape(theta[1:], [24, 24], order='F'))\n",
    "plt.title('Learned theta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Testing the Classifier [10 Points]\n",
    "\n",
    "**TODO**: Implement ***assignment1/predict_function.py*** according to specs. \n",
    "\n",
    "Test your implementation with the intial all zero theta as well! Does it match your expectation?\n",
    "\n",
    "***Hint***: All the methods should score above 90% on both the test and train set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final classifiers\n",
    "methods = ['sgd', 'newton', 'gda']\n",
    "\n",
    "for method in methods:\n",
    "    print('Evaluating {}\\n'.format(method))\n",
    "    start = time.time()\n",
    "\n",
    "    if method == 'sgd':\n",
    "        theta, losses = logistic_SGD(X_train, y_train)\n",
    "    elif method == 'newton':\n",
    "        theta, losses = logistic_Newton(X_train, y_train)\n",
    "    elif method == 'gda':\n",
    "        theta, losses = gda(X_train, y_train)\n",
    "    else:\n",
    "        raise ValueError('Method not recognised!')\n",
    "\n",
    "    exec_time = time.time()-start\n",
    "    print('Total execution time: {}s'.format(exec_time))\n",
    "\n",
    "    pred_test, accuracy_test = predict_function(theta, X_test, y_test)\n",
    "    pred_train, accuracy_train = predict_function(theta, X_train, y_train)\n",
    "    print('\\nTest accuracy: {}'.format(accuracy_test))\n",
    "    print('Training accuracy: {}\\n'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Discussion [20 Points]\n",
    "\n",
    "Answer the following questions and justify your answers:\n",
    "\n",
    "* Based on your results, which classifier do you prefer and why?\n",
    "\n",
    "\t***Your Answer:***\n",
    "    \n",
    "\n",
    "* SGD vs. Newton: Give advantages and disadvantages of both approaches. How would you expect the results to change if more or less data were used during training?\n",
    "\n",
    "\t***Your Answer:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Make it fast! [15 Points]\n",
    "\n",
    "Optimise your implementations by making good use of Numpy. If your implementation of one of the methods is within a factor of two of the reference implementation you'll earn 5 additional points (a total of 15 points possible for the three methods).\n",
    "\n",
    "***Hint***: No method should be significantly slower than the others in the case of optimal implementation. On my MBP they all take less than 2 seconds."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
