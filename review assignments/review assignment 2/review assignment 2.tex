% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx} %This allows to include eps figures
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage{layout}
\usepackage{etoolbox}
\usepackage{mathabx}
% This is to include code
\usepackage{listings}
\usepackage{xcolor}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstdefinestyle{Python}{
    language        = Python,
    basicstyle      = \ttfamily,
    keywordstyle    = \color{blue},
    keywordstyle    = [2] \color{teal}, % just to check that it works
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

%\renewcommand{\qedsymbol}{\filledbox}

\title{Review Assignment 2}%replace X with the appropriate number
\author{Nalet Meinen \\ %replace with your name
Machine Learning
}

\maketitle

\section{Calculus review}

Recall that the Jacobian of a function $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$ is an $m \times n$ matrix of partial derivatives 

\begin{align*}
    D f(x) = 
    \begin{bmatrix}
        \frac{\partial f_1(x)}{\partial x_1}    &   \frac{\partial f_1(x)}{\partial x_2}    & \dots     &   \frac{\partial f_1(x)}{\partial x_n}  \\
        \frac{\partial f_2(x)}{\partial x_1}    &   \frac{\partial f_2(x)}{\partial x_2}    & \dots     &   \frac{\partial f_2(x)}{\partial x_3}  \\
        \vdots                                  &   \vdots                                  & \ddots    &   \vdots                                \\   
        \frac{\partial f_m(x)}{\partial x_1}    &   \frac{\partial f_m(x)}{\partial x_2}    & \dots     &   \frac{\partial f_m(x)}{\partial x_n}  \\
    \end{bmatrix}
\end{align*}

\noindent where $ x =[x_1 x_2 \dots x_n]^\intercal$ , $f(x)=[f_1(x) f_2(x) \dots f_m(x)]^\intercal$ and $\frac{\partial f_i(x)}{\partial x_j}$ is the partial derivative of the $i$-th output with respect to the $j$-th input. 
When $f$ is a scalar-valued function (i.e., when $f : \mathbb{R}^n \rightarrow \mathbb{R}$), the Jacobian $Df(x)$ is a $1 \times n$ matrix, i.e., it is a row vector. Its transpose is called the \textit{gradient} of the function 

\begin{equation}
    \nabla f(x) = D f(x)^\intercal
    \begin{bmatrix}
        \frac{\partial f_1(x)}{\partial x_1}  \\
        \frac{\partial f(x)}{\partial x_2}  \\
        \vdots                                \\   
        \frac{\partial f(x)}{\partial x_n}  \\
    \end{bmatrix}
\end{equation}

\noindent Also, recall that the \textbf{chain rule} is a tool to calculate gradients of function compositions.
Suppose $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable at $x$ and $g : \mathbb{R}^m \rightarrow \mathbb{R}^p$ is differentiable at $f(x)$.
Define the composition $h : \mathbb{R}^m \rightarrow \mathbb{R}^p$ by $h(z)= g(f(z))$. Then $h$ is differentiable at $x$, with Jacobian
\begin{equation}
    Dh(x) = Dg(z)\Bigr|_{\substack{z=f(x)}} Df(x). 
\end{equation}

\begin{enumerate}

    \item Consider the function $g : \mathbb{R}^m \rightarrow \mathbb{R}$ with $g(x)= x^\intercal x$. We can readily calculate the gradient $\nabla g(x)=2x$ by noticing that 
    
    \begin{equation}
        \forall j = 1, \dots, n \qquad \frac{\partial x^\intercal x}{\partial x_j} = \frac{\partial x^2 j}{\partial x_j} = 2x_j \rightarrow \nabla g(x) = 2x
    \end{equation}

    Consider also the function $a : \mathbb{R}^n \rightarrow \mathbb{R}^m$ with $a(x)= Ax$, and $A \in \mathbb{R}^{m \times n}$. The Jacobian of $a(x)$ is $Da(x)= A$. 
    Given this, answer the following questions by using the above definitions (show all the steps of your working) 

    \begin{enumerate}

        \item Consider the function $h : \mathbb{R}^n \rightarrow \mathbb{R}$ and $h(x)= x^\intercal Qx$, where $Q \in \mathbb{R}^{n \times n}$ is a symmetric matrix. Calculate $\nabla h(x)$ by using the product rule, the gradient of g in eq. (3), and the Jacobian of the linear function $a(x)$. 
        
        \noindent\rule{\linewidth}{1pt}
        
        We notice that $\nabla g(x)=2x$, therefore 

        \begin{align*}
            \nabla h(x) &= \frac{\partial x^\intercal Qx^\intercal}{\partial x} \\
                        &= 2Qx
        \end{align*}
        
        \item Consider the function $f : \mathbb{R}^n \rightarrow \mathbb{R}$, where $f(x)= \lVert Ax - b \rVert ^2$ , $A \in \mathbb{R}^{m \times n}$ , and $b \in \mathbb{R}^m $. Calculate $\nabla h(x)$ by using the chain rule in eq. (2), the gradient of g in eq. (3), and the Jacobian of the linear function $a(x)$. 
        
        \noindent\rule{\linewidth}{1pt}
        
        \item Consider a function $f : \mathbb{R}^n \rightarrow \mathbb{R}$. Suppose we have a matrix $A \in \mathbb{R}^{n \times m}$ and a vector $x \in \mathbb{R}^m $. Calculate $\nabla xf(Ax)$ as a function of $\nabla xf(x)$.
        
        \noindent\rule{\linewidth}{1pt}

        \begin{align*}
            \lVert Ax - b \rVert ^2 &= (Ax - b)^\intercal (Ax - b) \\
            &= x^\intercal A^\intercal Ax-2b^\intercal Ax+b^\intercal b \\
        \end{align*}

        \begin{align*}
            \nabla h(x) &= \frac{\partial x^\intercal A^\intercal Ax-2b^\intercal Ax^\intercal}{\partial x} \\
                        &= 2Qx
        \end{align*}
        
        \item Show that
        \begin{align*}
            \frac{\partial}{\partial X} \sum_{i=1}{n} \lambda_i = 1
        \end{align*}
        where $X \in \mathbb{R}^{m \times n}$ and has eigenvalues $\lambda_1 \dots \lambda_n$

        \noindent\rule{\linewidth}{1pt}

        \item Show that
        \begin{align*}
            \frac{\partial}{\partial X} \prod_{i=1}{n} \lambda_i = \textrm(det)(X)X^{-\intercal}
        \end{align*}
        where $X \in \mathbb{R}^{m \times n}$ and has eigenvalues $\lambda_1 \dots \lambda_n$

        \noindent\rule{\linewidth}{1pt}


    \end{enumerate}   
    
    \item Assume $A \in \mathbb{R}^{m \times n} $, $X \in \mathbb{R}^{m \times n} $, and $B \in \mathbb{R}^{m \times m} $. Show that $\nabla X \textrm{tr}(AX^\intercal B) = BA$
    
    \noindent\rule{\linewidth}{1pt}
    \begin{align*}
        BA &= \nabla X \textrm{tr}(AX^\intercal B) \\
           &= \nabla X \textrm{tr}(X^\intercal BA) & \textrm{we can get rid of the trace}\\
           &= ((BA)^\intercal )^\intercal  & \textrm{transpose of transpose eliminate each other}\\
           &= BA
    \end{align*}

    \item Solve the following equality constrained optimization problem
    \begin{align*}
        \underset{\textrm{max}}{x \in \mathbb{R}^n} x^\intercal Ax \qquad \textrm{subject to} \; b^\intercal x=1
    \end{align*}
    for a symmetric matrix $A \in \mathbb{S}^n $. Assume that $A$ is invertible and $b \neq 0$. 

    \noindent\rule{\linewidth}{1pt}

    A standard way of solving optimization problems with equality constraints is by
    forming the Lagrangian, an objective function that includes the equality constraints.
    The Lagrangian in this case is be given by

    \begin{align*}
        \mathcal{L}(x, \lambda) = x^\intercal Ax - \lambda(b^\intercal x - 1).
    \end{align*}

    The parameter $\lambda$ is called the Lagrangian multiplier associated with the equality
    constraint. It can be shown that for $x*$ to be an optimal solution to the problem,
    the gradient of the Lagrangian w.r.t. $x$ has to be zero at $x*$. That is,

    \begin{align*}
        \nabla_x (\mathcal{L}(x, \lambda)) = \nabla_x (x^\intercal Ax - \lambda b^\intercal x) \overset{!}{=} 2Ax - 2\lambda b \
    \end{align*}
    
    This shows that the only points which can be possibly maximize (or minimize)
    $b^\intercal Ax$ assuming $x^\intercal b = 1$ are the eigenvectors of $A$.

\end{enumerate}

\end{document}
