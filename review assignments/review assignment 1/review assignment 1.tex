% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx} %This allows to include eps figures
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage{layout}
\usepackage{etoolbox}
\usepackage{mathabx}
% This is to include code
\usepackage{listings}
\usepackage{xcolor}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstdefinestyle{Python}{
    language        = Python,
    basicstyle      = \ttfamily,
    keywordstyle    = \color{blue},
    keywordstyle    = [2] \color{teal}, % just to check that it works
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

%\renewcommand{\qedsymbol}{\filledbox}

\title{Review Assignment 1}%replace X with the appropriate number
\author{Nalet Meinen \\ %replace with your name
Machine Learning
}

\maketitle

\section{Linear algebra review}

\begin{enumerate}
    \item S = $\{v_1, ... , v_n\}$ be an orthogonal set of non-zero vectors in $\mathbb{R}^n$. Prove that the vectors in $S$ are linearly independent.

    \noindent\rule{\linewidth}{1pt}

    We assume a linear combination
    \begin{align*} 
        c_1 v_1 + c_2 v_2 + ... + c_k v_k = 0
    \end{align*}
    We want to show that
    \begin{align*} 
        c_1 = c_2 = ... = 0
    \end{align*}
    The dot product of $v_i$ for each $ i = 1,2, ... , k $:
    \begin{align*} 
        0 &= v_i \cdot 0 \\
          &= v_i \cdot (c_1 v_1 + c_2 v_2 + ... + c_k v_k) \\
          &= c_1 v_i \cdot v_1 + c_2 v_i \cdot v_2 + ... + c_k v_i \cdot v_k
    \end{align*}
    $S$ is an orthogonal set, we have $v_i \cdot v_j = 0 \, \textrm{if} \, i \neq j $, then we have:
    \begin{align*} 
        0 = c_i v_i \cdot v_i = c_i \|v_i\|^2
    \end{align*}
    $v_i$ is nonzero and length $\|v_i\|$ is nonzero, following that $c_i = 0$ \newline
    We conclude that $c_1 v_1 + c_2 v_2 + ... + c_k v_k = 0$ for every $ i = 1,2, ... , k $, so $S$ is \textbf{linearly independent}

    \item Given a square matrix $A \in \mathbb{R}^{n \times n}$ and a vector $x \in \mathbb{R}^n$ show that $x^\intercal Ax = x^\intercal ( \frac{1}{2} A + \frac{1}{2} A^\intercal )x$.
    
    \noindent\rule{\linewidth}{1pt}

    From theory we know that $(cA)^\intercal = cA^\intercal$. Therefore we can transform the term $x^\intercal Ax$

    \begin{align*}
        x^\intercal Ax \rightarrow (x^\intercal Ax)^\intercal \rightarrow x^\intercal A^\intercal x
    \end{align*}

    This is similar to the goal in parentheses.

    \begin{align*}
        x^\intercal A^\intercal x \rightarrow \frac{(x^\intercal Ax) + (x^\intercal A^\intercal x)}{2}
    \end{align*}

    By writing the divisor 2 outsite of the brackets we become

    \begin{align*}
        \frac{(x^\intercal Ax) + (x^\intercal A^\intercal x)}{2} \rightarrow 
        \frac{1}{2} (x^\intercal Ax + x^\intercal A^\intercal x) \rightarrow 
         x^\intercal ( \frac{1}{2} A + \frac{1}{2} A^\intercal ) x
    \end{align*}

    Finally
    \begin{align*}
        x^\intercal Ax = x^\intercal ( \frac{1}{2} A + \frac{1}{2} A^\intercal ) x
    \end{align*}


    \item Show that if $(A + B)^{-1} = A^{-1} + B^{-1}$ then $A B^{-1} A = B A^{-1} B$
    
    \noindent\rule{\linewidth}{1pt}

    We knew from theory $A A^{-1} = A^{-1} A = I$ where $I$ is equal to the identity matrix.

    \begin{align*}
        (A B^{-1} A)^{-1} (B A^{-1} B) &= I \\
        (A^{-1}  B A^{-1} ) (B A^{-1} B) &= I \quad | \textrm{premultiply by} \; A \\
        A (A^{-1}  B A^{-1} ) (B A^{-1} B) &= A I\\
        I B A^{-1} (B A^{-1} B) &= A I\\
         B A^{-1} (B A^{-1} B) &= A \quad | \textrm{premultiply by} \; B^{-1}  \\
         &\vdots \\
         B A^{-1} B &= A B^{-1} A
    \end{align*}

    \item Use the definition of trace to show that $\textrm{tr}(A + B) = \textrm{tr}A + \textrm{tr}B$, where $A, B \in \mathbb{R}^{n \times n}$
    
    \noindent\rule{\linewidth}{1pt}

    \begin{align*}
        \textrm{tr}(A) &= \sum_{i=1}^{n} a_{ii} \quad \textrm{if} \, A = \textrm{squared matrix} \\
        \textrm{tr}(A + B) &= \sum_{i=1}^{n} a_{ii} + b_{ii}\\
        \textrm{tr}(A + B) &= 
        \begin{bmatrix}
            a_{11} + b_{11}  &   \dots            &   \dots            \\
            \vdots           &   a_{22} + b_{22}  &   \vdots           \\  
            \dots            &   \dots            &   a_{nn} + b_{nn}  \\
        \end{bmatrix} \\
         &= ( a_{11} + b_{11}  ) + ( a_{22} + b_{22} ) + ( \dots ) + ( a_{nn} + b_{nn}  ) \\
         &= a_{11} + b_{11} + \dots +  a_{nn}  +  a_{22} + b_{22}  +  \dots   + b_{nn}   \\
         &= ( a_{11} + b_{11} + \dots +  a_{nn} ) + ( a_{22} + b_{22}  +  \dots   + b_{nn} )   \\
         &= \textrm{tr}(A) + \textrm{tr}(B)
    \end{align*}   

    \item Show that if $(\lambda_i, x_i)$ are the $i$-th eigenvalue and $i$-th eigenvector of a non-singular and symmetric matrix 
          $A \in \mathbb{R}^{n \times n}$, then $( \frac{1}{\lambda_i}, x_i)$ are the $i$-th eigenvalue and $i$-th eigenvector of $A^{_1}$.
          \textit{Hint: use the eigendecomposition of $A$}
    
    \noindent\rule{\linewidth}{1pt}

    \begin{align*}
        x_i y_i^\intercal = [x_i][y_i] = 
        \begin{bmatrix}
            x_1 y_1          &   x_1 y_2          &   \dots            \\
            x_2 y_1          &   \dots            &   \dots            \\
            \vdots           &   \vdots           &   \vdots            
        \end{bmatrix}
    \end{align*}
    
    For $A,B \in \mathbb{R}^{m \times n}$, $\textrm{rank}(A+B) \leq \textrm{rank}(A) + \textrm{rank}(B)$
    \begin{align*}
        \sum_{i=1}^m x_i - y_i^\intercal \leq m 
    \end{align*}   

    \item Show that $\textrm{rank}(A) \leq \textrm{min}\{m, n\}$, where $A, B \in \mathbb{R}^{m \times n}$
    
    \noindent\rule{\linewidth}{1pt}

    We know that column rank and row rank of any matrix is equal. Also we know that the column rank is at most
    equal to the number of columns and the row rank is at most equal to the number of rows. These two consideration implies
    that $\textrm(rank(A)) \leq \textrm{min}\{m, n\}.   

    \item In each of the following cases, state whether the real matrix A is guaranteed to be singular or not. Justify your answer in each case.
    
    \begin{enumerate}

        \item  $A \in \mathbb{R}^{(n+1) \times n}$ is a full rank matrix.
        
        \noindent\rule{\linewidth}{1pt}

        No. A non-singular matrices should be square.

        \item $|A| = 0$.
        
        \noindent\rule{\linewidth}{1pt}

        No. An square matrix $A$ is non-singular if and only $|A| \neq 0$

        \item $A$ is an orthogonal matrix.
        
        \noindent\rule{\linewidth}{1pt} 
        
        Yes, For an orthogonal matrix $Q$ we have $Q^\intercal Q = QQ^\intercal = I$ so $Q$ is non singular and $Q^{-1} = Q^\intercal$

        \item $A$ has no eigenvalue equal to zero.
        
        \noindent\rule{\linewidth}{1pt}

        Yes, we know that $|A| = \prod \lambda_i$. So if $A$ has no zero eigenvalue, then $|A| \neq 0$ so $A$ is non-singular.

        \item $A$ is a symmetric matrix with non-negative eigenvalues.
        
        \noindent\rule{\linewidth}{1pt}

        No. We know that if $A$ is a symmetric matrix then $x^\intercal A = \sum_{i=1}^{n}$ and it is positive/negative for any $x$ if and only if all the eigenvalues are positive/negative.

    \end{enumerate}
    
\end{enumerate}    

\end{document}
