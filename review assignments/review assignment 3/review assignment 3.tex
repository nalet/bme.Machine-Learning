% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx} %This allows to include eps figures
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage{layout}
\usepackage{etoolbox}
\usepackage{mathabx}
\usepackage{tikz}
\usepackage{tikz-qtree}
% This is to include code
\usepackage{listings}
\usepackage{xcolor}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstdefinestyle{Python}{
    language        = Python,
    basicstyle      = \ttfamily,
    keywordstyle    = \color{blue},
    keywordstyle    = [2] \color{teal}, % just to check that it works
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

%\renewcommand{\qedsymbol}{\filledbox}

\title{Review Assignment 1}%replace X with the appropriate number
\author{Nalet Meinen \\ %replace with your name
Machine Learning
}

\maketitle

\section{Probability theory review}

Solve each of the following problems and show all the steps of your working.

\begin{enumerate}
    \item Show that the covariance matrix is always symmetric and positive semidefinite.
     
    \noindent\rule{\linewidth}{1pt}
    The $(i; j)^{th}$ element of the covariance matrix $\Sigma$ is given by
    \begin{align*}
        \Sigma_{ij} = E[(X_i - \mu_i)(X_j - \mu_j )] = E[(X_j - \mu_j)(X_i - \mu_i)] = \Sigma_{ji}
    \end{align*}
    so that the covariance matrix is symmetric.

    For an arbitrary vector u,
    \begin{align*}
        u^\intercal \Sigma u &= u^\intercal E[(X-\mu)(X-\mu)^\intercal]u = E[(u^\intercal (X - \mu)(X - \mu)^\intercal) u] \\
        &= E[((X - \mu)^\intercal u)^\intercal (X-\mu)^\intercal u] = E[((X-\mu)^\intercal u)^2] \geq 0
    \end{align*}
    so that the covariance matrix is positive semi-definite.

    \item $X \in \mathbb{R}^n$ and $Y \in \mathbb{R}^m$ are independent random variables. Their expectations and covariances are $E[X]=0$, $\textrm{cov}[X] = I$, $E[Y] = \mu$ and $\textrm{cov}[Y] = \sigma I$, 
        where $I$ is the identity matrix of the appropriate size and s is a scalar. What is the expectation and covariance of the random variable $z = AX + Y$ , where $A \in \mathbb{R}^{m \times n}$? 
    
    \noindent\rule{\linewidth}{1pt}

    The expectation of Z can be obtained from the definition by applying the linearity of expectation,

    \begin{align*}
        E[Z] = E[AX+Y] = AE[x]+E[Y] = 0 +\mu = \mu
    \end{align*}

    The covariance of $Z$ is $\textrm{Cov}[Z] = E[ZZ^\intercal] - E[Z]E[Z]^\intercal = E[ZZ^\intercal] - \mu \mu^\intercal $. Substituting the definition of $Z$, we get the expression below.

    \begin{align*}
        E[ZZ^\intercal ] &= E[(AX + Y )(AX + Y )^\intercal ] = \\ 
               &= E[AXX^\intercal A^\intercal  + Y X^\intercal A^\intercal  + AXY^\intercal  + Y Y^\intercal ] = \\
               &= AE[XX^\intercal ]A^\intercal  + E[Y X^\intercal ]A^\intercal  + AE[XY^\intercal ] + E[Y Y^\intercal ] \\
    \end{align*}

    Here we can substitute $E[XX^\intercal ] = I$ and $E[Y Y^\intercal ] = \sigma I+\mu\mu^\intercal $. Because $X$ and $Y$ are independent, $E[XY^\intercal ] = E[X]E[Y^\intercal = 0$, similarly $E[Y X^\intercal ] = 0$. We get $E[ZZ^\intercal ] = AA^\intercal  + \sigma I + \mu\mu^\intercal $, therefore $\textrm{Cov}[Z] = AA^\intercal  + \sigma I$.
    
    \item Thomas and Viktor are friends. It is Friday night and Thomas does not have a phone. Viktor knows that there is a 2/3 probability that Thomas goes to the party to downtown. 
          There are 5 pubs in downtown and there is an equal probability of Thomas going to any of them if he goes to the party. Viktor already looked for Thomas in 4 of the bars. 
          What is the probability of Viktor ending Thomas in the last bar? 
    
    \noindent\rule{\linewidth}{1pt}

    The sample space is
    \begin{align*}
        S = f\{\textrm{home}, \textrm{pub 1}, \textrm{pub 2}, \textrm{pub 3}, \textrm{pub 4}, \textrm{pub 5}\}
    \end{align*}
    and the probability of the events are $P(\textrm{home}) = 1=3$ and $P(\textrm{pub} \; i) = \frac{2}{15}$. We need to compute $P(\textrm{pub 5}|\textrm{not in pub} \; 1 \dots 4)$.
    Using the Bayes rule,
    \begin{align*}
        P(\textrm{pub 5}|\textrm{not in pub} \; 1 \dots 4) &=\\
        \frac{P(\textrm{pub 5} \cap \textrm{not in pub} \; 1 \dots 4)}{P(\textrm{not in pub} \; 1 \dots 4)} &= \frac{\frac{2}{15}}{\frac{7}{15}} = \frac{2}{7}
    \end{align*}    

    \item Derive the mean for the Beta Distribution, which is defined as 
    \begin{equation}
        \textrm{Beta}(x|a, b)= \frac{1}{B(a, b) } a^{-1}(1 - x)b^{-1} 
    \end{equation}
    where $B(a, b), G(a)$ are Beta and Gamma functions respectively: 
    \begin{equation}
        B(a, b) \triangleq \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b) }
    \end{equation}
    and 
    \begin{equation}
        \Gamma(x) \triangleq \int_0^{\infty } u^{x-1} e^{-u} d u
    \end{equation}

    \textit{Hint: Use integration by parts. }

    \begin{align*}
        E(x) &= \frac{1}{\textrm{Beta}(a,b)} \int_0^1 x^a(1-x)^{b-1} dx \\
             &= \frac{\textrm{Beta}(a + 1,b)}{\textrm{Beta}(a,b)} \\
             &= \frac{\Gamma(a + 1)\Gamma(b)}{\Gamma(a+b+1)} \cdot \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \\
             &= \frac{a}{a+b} \cdot \frac{\Gamma(a)\Gamma(b)\Gamma(a+b)}{\Gamma(a)\Gamma(b)\Gamma(a+b)} \\
             &= \frac{a}{a+b}
    \end{align*}

    \noindent\rule{\linewidth}{1pt}
    \item Let $A \in \mathbb{R}^{n \times n}$ be a positive definite square matrix, $b \in \mathbb{R}^n$ , and $c$ be a scalar. Prove that
    \begin{align*}  
        \int_{x \in \mathbb{R}^n} e^{-\frac{1}{2} x^\intercal Ax-x^\intercal b-c } dx = \frac{(2\pi)^{n/2} |A|^{-1/2} }{e^{c{-\frac{1}{2}b^\intercal A^{-1} b}}}
    \end{align*}
    \textit{Hint: Use the fact that the integral of the Gaussian probability density function of a random variable with mean $\mu$ and covariance $\sum$ is 1.}

    \noindent\rule{\linewidth}{1pt}

    \begin{align*}
        \frac{(2\pi)^{n/2} |A|^{-1/2} }{e^{c{-\frac{1}{2}b^\intercal A^{-1} b}}} &= \int_{x \in \mathbb{R}^n} e^{-\frac{1}{2} x^\intercal Ax-x^\intercal b-c } dx \\
        &= \int_{x \in \mathbb{R}^n} e^{-\frac{1}{2} (x + A^{-1} b)^\intercal A(x+A^{-1} b) + \frac{1}{2} b^\intercal A^{-1} b -c } dx \\
        &= \sqrt{(2\pi)^n |\Sigma|} \cdot e^{\frac{1}{2} b^\intercal A^{-1} b -c} \int_{x \in \mathbb{R}^n} \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} e^{-\frac{1}{2} (x + A^{-1} b)^\intercal A(x+A^{-1} b) } dx \\
        &= \sqrt{(2\pi)^n |\Sigma|} \cdot e^{\frac{1}{2} b^\intercal A^{-1} b -c} \cdot 1 \\
        &= \frac{\sqrt{(2\pi)^n |\Sigma|}}{e^{\frac{1}{2} b^\intercal A^{-1} b -c}}
    \end{align*}
    
    \item From the definition of conditional probability of multiple random variables, show that 
    \begin{align*}
        f(x_1,x_2, \dots x_n) = f(x_1) \prod_{i=2}^{n} f(x_i | x_1, \dots x_{i-1}) \\ 
    \end{align*}
    where $x_1, \dots x_n$ are random variables and $f$ is a probability density function of its arguments. 

    \noindent\rule{\linewidth}{1pt}

    \begin{align*}
        P(x_1 \cap x_2 \cap \dots \cap x_n) = P(x_1) \prod_{i=2}^{n} P(x_i | x_1, \dots x_{i-1}) 
    \end{align*}

    When $n=2$
    \begin{align*}
        P(x_1 \cap x_2) &= P(x_1) \prod_{i=2}^{n} P(x_2 | x_1) \\
                        &= P(x_1) P(x_2 | x_1)
    \end{align*}
        
    \begin{align*}
        P(x_1 \cap x_2 \cap \dots \cap x_{n-1} \cap x_n) &= P(x_n | x_1 \cap x_2 \cap \dots \cap x_{n-1} \cap x_n) P(x_1 \cap x_2 \cap \dots \cap x_{n-1} \cap x_n)
    \end{align*}

\end{enumerate}

\end{document}
